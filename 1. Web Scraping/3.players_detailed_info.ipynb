{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2197/2197\r"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "#Initialize\n",
    "team_id_pattern = r\"\\/verein\\/(\\d+)\\/saison_id\\/\\d+\"\n",
    "player_id_pattern = r\"\\/profil\\/spieler\\/(\\d+)\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def cleaner(string:str)->str:\n",
    "    string = string.replace(\"\\xa0\",\" \").replace(\"\\n\",\" \").replace(\"  \",\"\")\n",
    "    return string.strip()\n",
    "\n",
    "def team_id_extractor(url_inp:str)->str:\n",
    "    match = re.search(team_id_pattern, url_inp)\n",
    "    team_id = '0'\n",
    "    if match:\n",
    "        team_id = match.group(1)\n",
    "    return(team_id)\n",
    "\n",
    "def player_id_extractor(url_inp:str)->str:\n",
    "    match = re.search(player_id_pattern, url_inp)\n",
    "    player_id = '0'\n",
    "    if match:\n",
    "        player_id = match.group(1)\n",
    "    return(player_id)\n",
    "\n",
    "with open('links.txt', 'r') as file:\n",
    "    urls = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "\n",
    "# Initialize table players_ex_gk_detailed\n",
    "columns_players_ex_gk_detailed = ['Player_id','Season','Competition','Squad','Apperance','PPG','Goals','Assists','Own_goal','Substitutions_on','Substitutions_off','Yellow_card','Second_yellow_card','Red_card','Penalty_goals','Minutes_per_goal','Minutes_played','Club','Team_id']\n",
    "Players_ex_gk_detailed_table = pd.DataFrame(columns=columns_players_ex_gk_detailed)\n",
    "total_apperance_info_players = pd.DataFrame(columns=columns_players_ex_gk_detailed[3:-2])  \n",
    "columns_players_ex_gk_detailed.remove('Player_id')\n",
    "# To handle the creation of two empty columns that may occur during scraping, we can temporarily add two columns with the names \"temp1\" and \"temp2\". After creating the DataFrame table, we will drop these columns.\n",
    "columns_players_ex_gk_detailed.insert(1, 'temp1')\n",
    "columns_players_ex_gk_detailed.insert(3, 'temp2')\n",
    "\n",
    "# Initialize table gk_detailed\n",
    "columns_gk_detailed=['Player_id','Season','Competition','Squad','Apperance','PPG','Goals','Own_goal','Substitutions_on','Substitutions_off','Yellow_card','Second_yellow_card','Red_card','Goals_conceded','Clean_sheets','Minutes_played','Club','Team_id']\n",
    "Gk_detailed_table = pd.DataFrame(columns=columns_gk_detailed)\n",
    "total_apperance_info_gks = pd.DataFrame(columns=columns_gk_detailed[3:-2])  \n",
    "columns_gk_detailed.remove('Player_id')\n",
    "# To handle the creation of two empty columns that may occur during scraping, we can temporarily add two columns with the names \"temp1\" and \"temp2\". After creating the DataFrame table, we will drop these columns.\n",
    "columns_gk_detailed.insert(1, 'temp1')\n",
    "columns_gk_detailed.insert(3, 'temp2')\n",
    "\n",
    "max_retries = 5\n",
    "\n",
    "\n",
    "for count,url in enumerate((urls)):\n",
    "    print(f\"{count+1}/{len(urls)}\", end=\"\\r\")\n",
    "    player_id = player_id_extractor(url)\n",
    "    \n",
    "    record = {}\n",
    "        \n",
    "    #To create a URL for a details page\n",
    "    url = url.replace(\"profil\", \"leistungsdatendetails\")+ \"/saison//verein/0/liga/0/wettbewerb//pos/0/trainer_id/0/plus/1\"\n",
    "    \n",
    "    retries = 0\n",
    "    success = False\n",
    "    # Retry the request until success or maximum retries reached\n",
    "    while retries < max_retries and not success:\n",
    "        try:\n",
    "            html_content = requests.get(url,headers=HEADERS).text\n",
    "            success = True\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Error occurred while making a request. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "            retries += 1\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    # Find the table element using its class or id\n",
    "    position = 'Non-GK'\n",
    "    for li in soup.find_all('div' , class_='data-header__info-box')[0].find_all('li' , class_='data-header__label') :\n",
    "        if 'Position:' in li.text:\n",
    "            position = cleaner(li.find('span').text)\n",
    "    \n",
    "    table = soup.find_all(\"table\")[1]\n",
    "    \n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        try:\n",
    "            team_name = tr.find_all(\"td\", class_=\"hauptlink no-border-rechts zentriert\")[0].find(\"a\").get('title')\n",
    "            team_link = tr.find_all(\"td\", class_=\"hauptlink no-border-rechts zentriert\")[0].find(\"a\").get('href')\n",
    "        except:\n",
    "            team_name = 0\n",
    "            team_link = '0'\n",
    "        \n",
    "        match = re.search(team_id_pattern, team_link)\n",
    "        team_id = 0\n",
    "        if match:\n",
    "            team_id = match.group(1)   \n",
    "        \n",
    "        cells = []\n",
    "        for td in tr.find_all(\"td\"):\n",
    "            cells.append(td.text.strip())\n",
    "        cells.append(team_name)\n",
    "        cells.append(team_id)      \n",
    "        rows.append(cells)\n",
    "    \n",
    "    if position == 'Goalkeeper':\n",
    "        df = pd.DataFrame(rows, columns=columns_gk_detailed)\n",
    "    else:\n",
    "        df = pd.DataFrame(rows, columns=columns_players_ex_gk_detailed)              \n",
    "    \n",
    "    # Drop useless columns and row  \n",
    "    df = df.drop(columns='temp1')\n",
    "    df = df.drop(columns='temp2')\n",
    "    df = df.drop(index=0)\n",
    "    total_inf = df.iloc[0, 2:-2]\n",
    "    total_inf = total_inf.append(pd.Series([player_id]))\n",
    "    # Drop the total information from the detailed table of the player.\n",
    "    df = df.drop(index=1)\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    new_col_values = [player_id] * num_rows\n",
    "    df.insert(0, 'Player_id', new_col_values)\n",
    "    \n",
    "    if position == 'Goalkeeper':\n",
    "        Gk_detailed_table = Gk_detailed_table.append(df, ignore_index=True)\n",
    "        total_apperance_info_gks = total_apperance_info_gks.append(total_inf, ignore_index=True)\n",
    "    else:\n",
    "        Players_ex_gk_detailed_table = Players_ex_gk_detailed_table.append(df, ignore_index=True)\n",
    "        total_apperance_info_players = total_apperance_info_players.append(total_inf, ignore_index=True)\n",
    "total_apperance_info_players = total_apperance_info_players.rename(columns={0: 'player_id'})\n",
    "total_apperance_info_gks = total_apperance_info_gks.rename(columns={0: 'player_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_apperance_info_players.to_json('total_apperance_info_players.json', orient='records', indent=4)\n",
    "total_apperance_info_gks.to_json('total_apperance_info_gks.json', orient='records', indent=4)\n",
    "Gk_detailed_table.to_json('Gk_detailed_table.json', orient='records', indent=4)\n",
    "Players_ex_gk_detailed_table.to_json('Players_ex_gk_detailed_table.json', orient='records', indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "qenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
